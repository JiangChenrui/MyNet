    ,name                                              ,class_name             ,input_shape          ,output_shape         ,nb_params
1   ,conv1.0                                           ,Conv2d                 ,"(-1, 3, 224, 224)"  ,"(-1, 64, 112, 112)" ,tensor(1728)
2   ,conv1.1                                           ,BatchNorm2d            ,"(-1, 64, 112, 112)" ,"(-1, 64, 112, 112)" ,tensor(128)
3   ,conv1.2                                           ,ReLU6                  ,"(-1, 64, 112, 112)" ,"(-1, 64, 112, 112)" ,0
4   ,conv2.0                                           ,Conv2d                 ,"(-1, 64, 112, 112)" ,"(-1, 64, 56, 56)"   ,tensor(576)
5   ,conv2.1                                           ,BatchNorm2d            ,"(-1, 64, 56, 56)"   ,"(-1, 64, 56, 56)"   ,tensor(128)
6   ,conv2.2                                           ,ReLU6                  ,"(-1, 64, 56, 56)"   ,"(-1, 64, 56, 56)"   ,0
7   ,conv2.3                                           ,Conv2d                 ,"(-1, 64, 56, 56)"   ,"(-1, 128, 56, 56)"  ,tensor(8192)
8   ,conv2.4                                           ,BatchNorm2d            ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(256)
9   ,conv2.5                                           ,ReLU6                  ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
10  ,AttentionStage1.first_residual_blocks.0           ,Conv2d                 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(1152)
11  ,AttentionStage1.first_residual_blocks.1           ,BatchNorm2d            ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(256)
12  ,AttentionStage1.first_residual_blocks.2           ,ReLU6                  ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
13  ,AttentionStage1.first_residual_blocks.3           ,Conv2d                 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(16384)
14  ,AttentionStage1.first_residual_blocks.4           ,BatchNorm2d            ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(256)
15  ,AttentionStage1.first_residual_blocks.5           ,ReLU6                  ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
16  ,AttentionStage1.trunk_branches.0.0                ,Conv2d                 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(1152)
17  ,AttentionStage1.trunk_branches.0.1                ,BatchNorm2d            ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(256)
18  ,AttentionStage1.trunk_branches.0.2                ,ReLU6                  ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
19  ,AttentionStage1.trunk_branches.0.3                ,Conv2d                 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(16384)
20  ,AttentionStage1.trunk_branches.0.4                ,BatchNorm2d            ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(256)
21  ,AttentionStage1.trunk_branches.0.5                ,ReLU6                  ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
22  ,AttentionStage1.trunk_branches.1.0                ,Conv2d                 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(1152)
23  ,AttentionStage1.trunk_branches.1.1                ,BatchNorm2d            ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(256)
24  ,AttentionStage1.trunk_branches.1.2                ,ReLU6                  ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
25  ,AttentionStage1.trunk_branches.1.3                ,Conv2d                 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(16384)
26  ,AttentionStage1.trunk_branches.1.4                ,BatchNorm2d            ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(256)
27  ,AttentionStage1.trunk_branches.1.5                ,ReLU6                  ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
28  ,AttentionStage1.mpool1                            ,MaxPool2d              ,"(-1, 128, 56, 56)"  ,"(-1, 128, 28, 28)"  ,0
29  ,AttentionStage1.softmax1_blocks.0                 ,Conv2d                 ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(1152)
30  ,AttentionStage1.softmax1_blocks.1                 ,BatchNorm2d            ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(256)
31  ,AttentionStage1.softmax1_blocks.2                 ,ReLU6                  ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,0
32  ,AttentionStage1.softmax1_blocks.3                 ,Conv2d                 ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(16384)
33  ,AttentionStage1.softmax1_blocks.4                 ,BatchNorm2d            ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(256)
34  ,AttentionStage1.softmax1_blocks.5                 ,ReLU6                  ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,0
35  ,AttentionStage1.skip1_connection_residual_block.0 ,Conv2d                 ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(1152)
36  ,AttentionStage1.skip1_connection_residual_block.1 ,BatchNorm2d            ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(256)
37  ,AttentionStage1.skip1_connection_residual_block.2 ,ReLU6                  ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,0
38  ,AttentionStage1.skip1_connection_residual_block.3 ,Conv2d                 ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(16384)
39  ,AttentionStage1.skip1_connection_residual_block.4 ,BatchNorm2d            ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(256)
40  ,AttentionStage1.skip1_connection_residual_block.5 ,ReLU6                  ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,0
41  ,AttentionStage1.mpool2                            ,MaxPool2d              ,"(-1, 128, 28, 28)"  ,"(-1, 128, 14, 14)"  ,0
42  ,AttentionStage1.softmax2_blocks.0                 ,Conv2d                 ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(1152)
43  ,AttentionStage1.softmax2_blocks.1                 ,BatchNorm2d            ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(256)
44  ,AttentionStage1.softmax2_blocks.2                 ,ReLU6                  ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,0
45  ,AttentionStage1.softmax2_blocks.3                 ,Conv2d                 ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(16384)
46  ,AttentionStage1.softmax2_blocks.4                 ,BatchNorm2d            ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(256)
47  ,AttentionStage1.softmax2_blocks.5                 ,ReLU6                  ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,0
48  ,AttentionStage1.skip2_connection_residual_block.0 ,Conv2d                 ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(1152)
49  ,AttentionStage1.skip2_connection_residual_block.1 ,BatchNorm2d            ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(256)
50  ,AttentionStage1.skip2_connection_residual_block.2 ,ReLU6                  ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,0
51  ,AttentionStage1.skip2_connection_residual_block.3 ,Conv2d                 ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(16384)
52  ,AttentionStage1.skip2_connection_residual_block.4 ,BatchNorm2d            ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(256)
53  ,AttentionStage1.skip2_connection_residual_block.5 ,ReLU6                  ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,0
54  ,AttentionStage1.mpool3                            ,MaxPool2d              ,"(-1, 128, 14, 14)"  ,"(-1, 128, 7, 7)"    ,0
55  ,AttentionStage1.softmax3_blocks.0.0               ,Conv2d                 ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,tensor(1152)
56  ,AttentionStage1.softmax3_blocks.0.1               ,BatchNorm2d            ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,tensor(256)
57  ,AttentionStage1.softmax3_blocks.0.2               ,ReLU6                  ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,0
58  ,AttentionStage1.softmax3_blocks.0.3               ,Conv2d                 ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,tensor(16384)
59  ,AttentionStage1.softmax3_blocks.0.4               ,BatchNorm2d            ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,tensor(256)
60  ,AttentionStage1.softmax3_blocks.0.5               ,ReLU6                  ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,0
61  ,AttentionStage1.softmax3_blocks.1.0               ,Conv2d                 ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,tensor(1152)
62  ,AttentionStage1.softmax3_blocks.1.1               ,BatchNorm2d            ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,tensor(256)
63  ,AttentionStage1.softmax3_blocks.1.2               ,ReLU6                  ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,0
64  ,AttentionStage1.softmax3_blocks.1.3               ,Conv2d                 ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,tensor(16384)
65  ,AttentionStage1.softmax3_blocks.1.4               ,BatchNorm2d            ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,tensor(256)
66  ,AttentionStage1.softmax3_blocks.1.5               ,ReLU6                  ,"(-1, 128, 7, 7)"    ,"(-1, 128, 7, 7)"    ,0
67  ,AttentionStage1.interpolation3                    ,UpsamplingBilinear2d   ,"(-1, 128, 7, 7)"    ,"(-1, 128, 14, 14)"  ,0
68  ,AttentionStage1.softmax4_blocks.0                 ,Conv2d                 ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(1152)
69  ,AttentionStage1.softmax4_blocks.1                 ,BatchNorm2d            ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(256)
70  ,AttentionStage1.softmax4_blocks.2                 ,ReLU6                  ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,0
71  ,AttentionStage1.softmax4_blocks.3                 ,Conv2d                 ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(16384)
72  ,AttentionStage1.softmax4_blocks.4                 ,BatchNorm2d            ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,tensor(256)
73  ,AttentionStage1.softmax4_blocks.5                 ,ReLU6                  ,"(-1, 128, 14, 14)"  ,"(-1, 128, 14, 14)"  ,0
74  ,AttentionStage1.interpolation2                    ,UpsamplingBilinear2d   ,"(-1, 128, 14, 14)"  ,"(-1, 128, 28, 28)"  ,0
75  ,AttentionStage1.softmax5_blocks.0                 ,Conv2d                 ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(1152)
76  ,AttentionStage1.softmax5_blocks.1                 ,BatchNorm2d            ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(256)
77  ,AttentionStage1.softmax5_blocks.2                 ,ReLU6                  ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,0
78  ,AttentionStage1.softmax5_blocks.3                 ,Conv2d                 ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(16384)
79  ,AttentionStage1.softmax5_blocks.4                 ,BatchNorm2d            ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(256)
80  ,AttentionStage1.softmax5_blocks.5                 ,ReLU6                  ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,0
81  ,AttentionStage1.interpolation1                    ,UpsamplingBilinear2d   ,"(-1, 128, 28, 28)"  ,"(-1, 128, 56, 56)"  ,0
82  ,AttentionStage1.softmax6_blocks.0                 ,BatchNorm2d            ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(256)
83  ,AttentionStage1.softmax6_blocks.1                 ,ReLU                   ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
84  ,AttentionStage1.softmax6_blocks.2                 ,Conv2d                 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(16384)
85  ,AttentionStage1.softmax6_blocks.3                 ,BatchNorm2d            ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(256)
86  ,AttentionStage1.softmax6_blocks.4                 ,ReLU                   ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
87  ,AttentionStage1.softmax6_blocks.5                 ,Conv2d                 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(16384)
88  ,AttentionStage1.softmax6_blocks.6                 ,Sigmoid                ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
89  ,AttentionStage1.last_blocks.0                     ,Conv2d                 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(1152)
90  ,AttentionStage1.last_blocks.1                     ,BatchNorm2d            ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(256)
91  ,AttentionStage1.last_blocks.2                     ,ReLU6                  ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
92  ,AttentionStage1.last_blocks.3                     ,Conv2d                 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(16384)
93  ,AttentionStage1.last_blocks.4                     ,BatchNorm2d            ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(256)
94  ,AttentionStage1.last_blocks.5                     ,ReLU6                  ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,0
95  ,AttentionStage1                                   ,AttentionModule_stage1 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 56, 56)"  ,tensor(249856)
96  ,conv3.0                                           ,Conv2d                 ,"(-1, 128, 56, 56)"  ,"(-1, 128, 28, 28)"  ,tensor(1152)
97  ,conv3.1                                           ,BatchNorm2d            ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,tensor(256)
98  ,conv3.2                                           ,ReLU6                  ,"(-1, 128, 28, 28)"  ,"(-1, 128, 28, 28)"  ,0
99  ,conv3.3                                           ,Conv2d                 ,"(-1, 128, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(32768)
100 ,conv3.4                                           ,BatchNorm2d            ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(512)
101 ,conv3.5                                           ,ReLU6                  ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
102 ,AttentionStage2.first_residual_blocks.0           ,Conv2d                 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(2304)
103 ,AttentionStage2.first_residual_blocks.1           ,BatchNorm2d            ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(512)
104 ,AttentionStage2.first_residual_blocks.2           ,ReLU6                  ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
105 ,AttentionStage2.first_residual_blocks.3           ,Conv2d                 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(65536)
106 ,AttentionStage2.first_residual_blocks.4           ,BatchNorm2d            ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(512)
107 ,AttentionStage2.first_residual_blocks.5           ,ReLU6                  ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
108 ,AttentionStage2.trunk_branches.0.0                ,Conv2d                 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(2304)
109 ,AttentionStage2.trunk_branches.0.1                ,BatchNorm2d            ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(512)
110 ,AttentionStage2.trunk_branches.0.2                ,ReLU6                  ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
111 ,AttentionStage2.trunk_branches.0.3                ,Conv2d                 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(65536)
112 ,AttentionStage2.trunk_branches.0.4                ,BatchNorm2d            ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(512)
113 ,AttentionStage2.trunk_branches.0.5                ,ReLU6                  ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
114 ,AttentionStage2.trunk_branches.1.0                ,Conv2d                 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(2304)
115 ,AttentionStage2.trunk_branches.1.1                ,BatchNorm2d            ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(512)
116 ,AttentionStage2.trunk_branches.1.2                ,ReLU6                  ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
117 ,AttentionStage2.trunk_branches.1.3                ,Conv2d                 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(65536)
118 ,AttentionStage2.trunk_branches.1.4                ,BatchNorm2d            ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(512)
119 ,AttentionStage2.trunk_branches.1.5                ,ReLU6                  ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
120 ,AttentionStage2.mpool1                            ,MaxPool2d              ,"(-1, 256, 28, 28)"  ,"(-1, 256, 14, 14)"  ,0
121 ,AttentionStage2.softmax1_blocks.0                 ,Conv2d                 ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(2304)
122 ,AttentionStage2.softmax1_blocks.1                 ,BatchNorm2d            ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(512)
123 ,AttentionStage2.softmax1_blocks.2                 ,ReLU6                  ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,0
124 ,AttentionStage2.softmax1_blocks.3                 ,Conv2d                 ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(65536)
125 ,AttentionStage2.softmax1_blocks.4                 ,BatchNorm2d            ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(512)
126 ,AttentionStage2.softmax1_blocks.5                 ,ReLU6                  ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,0
127 ,AttentionStage2.skip1_connection_residual_block.0 ,Conv2d                 ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(2304)
128 ,AttentionStage2.skip1_connection_residual_block.1 ,BatchNorm2d            ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(512)
129 ,AttentionStage2.skip1_connection_residual_block.2 ,ReLU6                  ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,0
130 ,AttentionStage2.skip1_connection_residual_block.3 ,Conv2d                 ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(65536)
131 ,AttentionStage2.skip1_connection_residual_block.4 ,BatchNorm2d            ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(512)
132 ,AttentionStage2.skip1_connection_residual_block.5 ,ReLU6                  ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,0
133 ,AttentionStage2.mpool2                            ,MaxPool2d              ,"(-1, 256, 14, 14)"  ,"(-1, 256, 7, 7)"    ,0
134 ,AttentionStage2.softmax2_blocks.0.0               ,Conv2d                 ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,tensor(2304)
135 ,AttentionStage2.softmax2_blocks.0.1               ,BatchNorm2d            ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,tensor(512)
136 ,AttentionStage2.softmax2_blocks.0.2               ,ReLU6                  ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,0
137 ,AttentionStage2.softmax2_blocks.0.3               ,Conv2d                 ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,tensor(65536)
138 ,AttentionStage2.softmax2_blocks.0.4               ,BatchNorm2d            ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,tensor(512)
139 ,AttentionStage2.softmax2_blocks.0.5               ,ReLU6                  ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,0
140 ,AttentionStage2.softmax2_blocks.1.0               ,Conv2d                 ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,tensor(2304)
141 ,AttentionStage2.softmax2_blocks.1.1               ,BatchNorm2d            ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,tensor(512)
142 ,AttentionStage2.softmax2_blocks.1.2               ,ReLU6                  ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,0
143 ,AttentionStage2.softmax2_blocks.1.3               ,Conv2d                 ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,tensor(65536)
144 ,AttentionStage2.softmax2_blocks.1.4               ,BatchNorm2d            ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,tensor(512)
145 ,AttentionStage2.softmax2_blocks.1.5               ,ReLU6                  ,"(-1, 256, 7, 7)"    ,"(-1, 256, 7, 7)"    ,0
146 ,AttentionStage2.interpolation2                    ,UpsamplingBilinear2d   ,"(-1, 256, 7, 7)"    ,"(-1, 256, 14, 14)"  ,0
147 ,AttentionStage2.softmax3_blocks.0                 ,Conv2d                 ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(2304)
148 ,AttentionStage2.softmax3_blocks.1                 ,BatchNorm2d            ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(512)
149 ,AttentionStage2.softmax3_blocks.2                 ,ReLU6                  ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,0
150 ,AttentionStage2.softmax3_blocks.3                 ,Conv2d                 ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(65536)
151 ,AttentionStage2.softmax3_blocks.4                 ,BatchNorm2d            ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(512)
152 ,AttentionStage2.softmax3_blocks.5                 ,ReLU6                  ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,0
153 ,AttentionStage2.interpolation1                    ,UpsamplingBilinear2d   ,"(-1, 256, 14, 14)"  ,"(-1, 256, 28, 28)"  ,0
154 ,AttentionStage2.softmax4_blocks.0                 ,BatchNorm2d            ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(512)
155 ,AttentionStage2.softmax4_blocks.1                 ,ReLU                   ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
156 ,AttentionStage2.softmax4_blocks.2                 ,Conv2d                 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(65536)
157 ,AttentionStage2.softmax4_blocks.3                 ,BatchNorm2d            ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(512)
158 ,AttentionStage2.softmax4_blocks.4                 ,ReLU                   ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
159 ,AttentionStage2.softmax4_blocks.5                 ,Conv2d                 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(65536)
160 ,AttentionStage2.softmax4_blocks.6                 ,Sigmoid                ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
161 ,AttentionStage2.last_blocks.0                     ,Conv2d                 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(2304)
162 ,AttentionStage2.last_blocks.1                     ,BatchNorm2d            ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(512)
163 ,AttentionStage2.last_blocks.2                     ,ReLU6                  ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
164 ,AttentionStage2.last_blocks.3                     ,Conv2d                 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(65536)
165 ,AttentionStage2.last_blocks.4                     ,BatchNorm2d            ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(512)
166 ,AttentionStage2.last_blocks.5                     ,ReLU6                  ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,0
167 ,AttentionStage2                                   ,AttentionModule_stage2 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 28, 28)"  ,tensor(751872)
168 ,conv4.0                                           ,Conv2d                 ,"(-1, 256, 28, 28)"  ,"(-1, 256, 14, 14)"  ,tensor(2304)
169 ,conv4.1                                           ,BatchNorm2d            ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,tensor(512)
170 ,conv4.2                                           ,ReLU6                  ,"(-1, 256, 14, 14)"  ,"(-1, 256, 14, 14)"  ,0
171 ,conv4.3                                           ,Conv2d                 ,"(-1, 256, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(131072)
172 ,conv4.4                                           ,BatchNorm2d            ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(1024)
173 ,conv4.5                                           ,ReLU6                  ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
174 ,AttentionStage3.first_residual_blocks.0           ,Conv2d                 ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(4608)
175 ,AttentionStage3.first_residual_blocks.1           ,BatchNorm2d            ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(1024)
176 ,AttentionStage3.first_residual_blocks.2           ,ReLU6                  ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
177 ,AttentionStage3.first_residual_blocks.3           ,Conv2d                 ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(262144)
178 ,AttentionStage3.first_residual_blocks.4           ,BatchNorm2d            ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(1024)
179 ,AttentionStage3.first_residual_blocks.5           ,ReLU6                  ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
180 ,AttentionStage3.trunk_branches.0.0                ,Conv2d                 ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(4608)
181 ,AttentionStage3.trunk_branches.0.1                ,BatchNorm2d            ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(1024)
182 ,AttentionStage3.trunk_branches.0.2                ,ReLU6                  ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
183 ,AttentionStage3.trunk_branches.0.3                ,Conv2d                 ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(262144)
184 ,AttentionStage3.trunk_branches.0.4                ,BatchNorm2d            ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(1024)
185 ,AttentionStage3.trunk_branches.0.5                ,ReLU6                  ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
186 ,AttentionStage3.trunk_branches.1.0                ,Conv2d                 ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(4608)
187 ,AttentionStage3.trunk_branches.1.1                ,BatchNorm2d            ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(1024)
188 ,AttentionStage3.trunk_branches.1.2                ,ReLU6                  ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
189 ,AttentionStage3.trunk_branches.1.3                ,Conv2d                 ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(262144)
190 ,AttentionStage3.trunk_branches.1.4                ,BatchNorm2d            ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(1024)
191 ,AttentionStage3.trunk_branches.1.5                ,ReLU6                  ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
192 ,AttentionStage3.mpool1                            ,MaxPool2d              ,"(-1, 512, 14, 14)"  ,"(-1, 512, 7, 7)"    ,0
193 ,AttentionStage3.softmax1_blocks.0.0               ,Conv2d                 ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,tensor(4608)
194 ,AttentionStage3.softmax1_blocks.0.1               ,BatchNorm2d            ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,tensor(1024)
195 ,AttentionStage3.softmax1_blocks.0.2               ,ReLU6                  ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,0
196 ,AttentionStage3.softmax1_blocks.0.3               ,Conv2d                 ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,tensor(262144)
197 ,AttentionStage3.softmax1_blocks.0.4               ,BatchNorm2d            ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,tensor(1024)
198 ,AttentionStage3.softmax1_blocks.0.5               ,ReLU6                  ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,0
199 ,AttentionStage3.softmax1_blocks.1.0               ,Conv2d                 ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,tensor(4608)
200 ,AttentionStage3.softmax1_blocks.1.1               ,BatchNorm2d            ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,tensor(1024)
201 ,AttentionStage3.softmax1_blocks.1.2               ,ReLU6                  ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,0
202 ,AttentionStage3.softmax1_blocks.1.3               ,Conv2d                 ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,tensor(262144)
203 ,AttentionStage3.softmax1_blocks.1.4               ,BatchNorm2d            ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,tensor(1024)
204 ,AttentionStage3.softmax1_blocks.1.5               ,ReLU6                  ,"(-1, 512, 7, 7)"    ,"(-1, 512, 7, 7)"    ,0
205 ,AttentionStage3.interpolation1                    ,UpsamplingBilinear2d   ,"(-1, 512, 7, 7)"    ,"(-1, 512, 14, 14)"  ,0
206 ,AttentionStage3.softmax2_blocks.0                 ,BatchNorm2d            ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(1024)
207 ,AttentionStage3.softmax2_blocks.1                 ,ReLU                   ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
208 ,AttentionStage3.softmax2_blocks.2                 ,Conv2d                 ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(262144)
209 ,AttentionStage3.softmax2_blocks.3                 ,BatchNorm2d            ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(1024)
210 ,AttentionStage3.softmax2_blocks.4                 ,ReLU                   ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
211 ,AttentionStage3.softmax2_blocks.5                 ,Conv2d                 ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(262144)
212 ,AttentionStage3.softmax2_blocks.6                 ,Sigmoid                ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
213 ,AttentionStage3.last_blocks.0                     ,Conv2d                 ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(4608)
214 ,AttentionStage3.last_blocks.1                     ,BatchNorm2d            ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(1024)
215 ,AttentionStage3.last_blocks.2                     ,ReLU6                  ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
216 ,AttentionStage3.last_blocks.3                     ,Conv2d                 ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(262144)
217 ,AttentionStage3.last_blocks.4                     ,BatchNorm2d            ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(1024)
218 ,AttentionStage3.last_blocks.5                     ,ReLU6                  ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,0
219 ,AttentionStage3                                   ,AttentionModule_stage3 ,"(-1, 512, 14, 14)"  ,"(-1, 512, 14, 14)"  ,tensor(2139136)
220 ,pool                                              ,AvgPool2d              ,"(-1, 512, 14, 14)"  ,"(-1, 512, 2, 2)"    ,0
221 ,fc.0                                              ,Dropout                ,"(-1, 512)"          ,"(-1, 512)"          ,0
222 ,fc.1                                              ,Linear                 ,"(-1, 512)"          ,"(-1, 4)"            ,tensor(2052)
