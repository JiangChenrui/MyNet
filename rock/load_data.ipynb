{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = 'data/train_old'\n",
    "val_img_path = 'data/test_old'\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_path(img_path):\n",
    "    _path = []\n",
    "    lables = []\n",
    "    with open(img_path, 'r') as file:\n",
    "        # 将文本文件的数据读入到path_data\n",
    "        path_data = file.readlines()\n",
    "        for i in range(0, len(path_data)):\n",
    "            # 将字符串格式转换为正常格式\n",
    "            path_data[i] = path_data[i].split()\n",
    "            _path.append(path_data[i][0])\n",
    "            lables.append(int(path_data[i][1]))\n",
    "        x = np.array(_path)\n",
    "        y = np.array(lables)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_path(train_img_path)\n",
    "data_num = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_data(path, size=None):\n",
    "    if size != None:\n",
    "        return cv2.resize(cv2.imread(path), size)\n",
    "    else:\n",
    "        return cv2.resize(cv2.imread(path), (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_batch(x, y, batch_size=128):\n",
    "    for start in range(0, data_num):\n",
    "        x_batch = x[start: start + batch_size]\n",
    "        y_batch = y[start: start + batch_size]\n",
    "        start += batch_size\n",
    "\n",
    "        _x_batch = []\n",
    "        for i in range(x_batch.shape[0]):\n",
    "            _x_batch += [path_to_data(x_batch[i]), (224, 224)]\n",
    "        x_batch = np.array(_x_batch)\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object generate_batch at 0x7f588a27ad58>\n",
      "data/track_data_new/no_daocha/jinan_20150921_no_daocha_left_00273.jpg 0\n"
     ]
    }
   ],
   "source": [
    "print(generate_batch(x_train, y_train, 128))\n",
    "print(x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "取值范围整数个数： [5]\n",
      "     house  house  house  house  house\n",
      "0      1.0    0.0    0.0    0.0    0.0\n",
      "1      0.0    1.0    0.0    0.0    0.0\n",
      "2      1.0    0.0    0.0    0.0    0.0\n",
      "3      1.0    0.0    0.0    0.0    0.0\n",
      "4      1.0    0.0    0.0    0.0    0.0\n",
      "5      0.0    1.0    0.0    0.0    0.0\n",
      "6      0.0    1.0    0.0    0.0    0.0\n",
      "7      0.0    1.0    0.0    0.0    0.0\n",
      "8      1.0    0.0    0.0    0.0    0.0\n",
      "9      0.0    1.0    0.0    0.0    0.0\n",
      "10     1.0    0.0    0.0    0.0    0.0\n",
      "11     0.0    1.0    0.0    0.0    0.0\n",
      "12     1.0    0.0    0.0    0.0    0.0\n",
      "13     0.0    1.0    0.0    0.0    0.0\n",
      "14     1.0    0.0    0.0    0.0    0.0\n",
      "15     0.0    1.0    0.0    0.0    0.0\n",
      "16     1.0    0.0    0.0    0.0    0.0\n",
      "17     1.0    0.0    0.0    0.0    0.0\n",
      "18     0.0    1.0    0.0    0.0    0.0\n",
      "19     1.0    0.0    0.0    0.0    0.0\n",
      "20     0.0    1.0    0.0    0.0    0.0\n",
      "21     0.0    1.0    0.0    0.0    0.0\n",
      "22     0.0    1.0    0.0    0.0    0.0\n",
      "23     1.0    0.0    0.0    0.0    0.0\n",
      "24     1.0    0.0    0.0    0.0    0.0\n",
      "25     0.0    1.0    0.0    0.0    0.0\n",
      "26     1.0    0.0    0.0    0.0    0.0\n",
      "27     0.0    1.0    0.0    0.0    0.0\n",
      "28     1.0    0.0    0.0    0.0    0.0\n",
      "29     1.0    0.0    0.0    0.0    0.0\n",
      "..     ...    ...    ...    ...    ...\n",
      "174    0.0    0.0    0.0    0.0    1.0\n",
      "175    0.0    1.0    0.0    0.0    0.0\n",
      "176    0.0    0.0    0.0    0.0    1.0\n",
      "177    0.0    1.0    0.0    0.0    0.0\n",
      "178    0.0    1.0    0.0    0.0    0.0\n",
      "179    0.0    0.0    0.0    1.0    0.0\n",
      "180    0.0    1.0    0.0    0.0    0.0\n",
      "181    0.0    1.0    0.0    0.0    0.0\n",
      "182    0.0    0.0    0.0    1.0    0.0\n",
      "183    0.0    0.0    1.0    0.0    0.0\n",
      "184    0.0    0.0    0.0    1.0    0.0\n",
      "185    0.0    0.0    1.0    0.0    0.0\n",
      "186    0.0    0.0    0.0    1.0    0.0\n",
      "187    0.0    0.0    1.0    0.0    0.0\n",
      "188    0.0    0.0    0.0    1.0    0.0\n",
      "189    0.0    0.0    1.0    0.0    0.0\n",
      "190    0.0    0.0    1.0    0.0    0.0\n",
      "191    0.0    0.0    1.0    0.0    0.0\n",
      "192    0.0    0.0    1.0    0.0    0.0\n",
      "193    0.0    0.0    0.0    0.0    1.0\n",
      "194    0.0    0.0    0.0    0.0    1.0\n",
      "195    0.0    1.0    0.0    0.0    0.0\n",
      "196    0.0    0.0    1.0    0.0    0.0\n",
      "197    0.0    0.0    1.0    0.0    0.0\n",
      "198    0.0    1.0    0.0    0.0    0.0\n",
      "199    0.0    0.0    1.0    0.0    0.0\n",
      "200    0.0    0.0    0.0    0.0    1.0\n",
      "201    0.0    0.0    1.0    0.0    0.0\n",
      "202    0.0    0.0    1.0    0.0    0.0\n",
      "203    0.0    0.0    1.0    0.0    0.0\n",
      "\n",
      "[204 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, confusion_matrix\n",
    "data = pd.read_csv('data.csv')\n",
    "D = 'D'\n",
    "N = 'N'\n",
    "ROM = 'ROM'\n",
    "L = 'L'\n",
    "V = 'V'\n",
    "rock = 'rock'\n",
    "\n",
    "# 将带有中午属性的词转换为数值\n",
    "listUniq = data.ix[:,'drill'].unique()\n",
    "for j in range(len(listUniq)):\n",
    "    data.ix[:,'drill'] = data.ix[:,'drill'].apply(lambda x:j if x==listUniq[j] else x)\n",
    "listUniq = data.ix[:,'rock'].unique()\n",
    "for j in range(len(listUniq)):\n",
    "    data.ix[:,'rock'] = data.ix[:,'rock'].apply(lambda x:j if x==listUniq[j] else x)\n",
    "    \n",
    "# 进行one_hot编码\n",
    "tempdata = data[['drill']]\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(tempdata)\n",
    "\n",
    "# 先将one-hot编码的结果是比较奇怪的，最好是先转换成二维数组\n",
    "tempdata = enc.transform(tempdata).toarray()\n",
    "print(tempdata)\n",
    "print('取值范围整数个数：',enc.n_values_)\n",
    "\n",
    "#再将二维数组转换为DataFrame，记得这里会变成多列\n",
    "tempdata = pd.DataFrame(tempdata,columns=['house']*len(tempdata[0]))\n",
    "print(tempdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   1.   0.   ... 0.87 2.31 1.77]\n",
      " [0.   0.   0.   ... 3.28 1.06 2.5 ]\n",
      " [1.   1.   0.   ... 0.13 1.84 2.03]\n",
      " ...\n",
      " [0.   1.   0.   ... 2.83 3.19 3.06]\n",
      " [1.   0.   1.   ... 2.28 6.85 0.92]\n",
      " [0.   1.   1.   ... 2.87 2.17 3.32]]\n",
      "[[0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 2 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0\n",
      "  1 3 3 1 1 3 3 3 1 3 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 2 1 1 1 1 1 0 0 0\n",
      "  1 1 1 1 1 3 1 1 0 0 0 1 1 0 1 1 0 0 0 2 1 2 1 1 0 0 3 3 3 1 1 1 0 0 0 0\n",
      "  0 2 1 0 2 0 1 0 0 0 3 0 2 0 0 3 2 1 2 0 1 3 1 0 1 3 2 1 3 3 3 3 3 0 0 0\n",
      "  1 1 1 0 0 0 0 0 2 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 3 3 3 0 1 1\n",
      "  0 0 0 0 0 0 0 0 0 0 1 1 1 2 1 1 1 0 0 0 2 1 3 1]]\n"
     ]
    }
   ],
   "source": [
    "x_columns = [x for x in data.columns if x in [D, N, ROM, L, V]]\n",
    "X = np.array(pd.concat([tempdata, data[x_columns]],axis=1))\n",
    "# X = np.array(data[x_columns])\n",
    "X.resize([X.shape[1], 204])\n",
    "y = np.array([data[rock]])\n",
    "means = np.mean(X)\n",
    "stedvs = np.std(X)\n",
    "# X = (X - means) / stedvs\n",
    "print(X)\n",
    "# X = (X - X.min()) / (X.max() - X.min())\n",
    "Y = np.array([data['rock']])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建神经网络结构\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        X ：输入数据集，维度为（输入的数量，训练/测试的数量）\n",
    "        Y ：标签，维度为（输出的数量，训练/测试数量）\n",
    "    \n",
    "    返回：\n",
    "        n_x ：输入层数量\n",
    "        n_y : 输出层数量\n",
    "        n_h : 隐藏层数量\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0] # 输入层\n",
    "    n_h = 4 # 隐藏层\n",
    "    n_y = Y.shape[0] #输出层\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型参数\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        n_x 输入节点的数量\n",
    "        n_h 隐藏节点的数量\n",
    "        n_y 输出节点的数量\n",
    "    返回：\n",
    "        parameters 包含参数的字典\n",
    "            W1 权重矩阵，维度为（n_h, n_x）\n",
    "            b1 偏向量，维度为（n_h, 1）\n",
    "            W2 权重矩阵，维度为（n_y, n_h）\n",
    "            b1 偏向量，维度为（n_y, 1）\n",
    "    \"\"\"\n",
    "    np.random.seed(2) # 指定一个随机种子\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01 # 随机初始化一个维度为（n_h, n_x）的矩阵\n",
    "    b1 = np.zeros(shape=(n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros(shape=(n_y, 1))\n",
    "    \n",
    "    # 使用断言判断格式是否正确\n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\" : W1,\n",
    "                  \"b1\" : b1,\n",
    "                  \"W2\" : W2,\n",
    "                  \"b2\" : b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        X 维度为（n_x, m）的输入数据\n",
    "        parameters 初始化函数（initialize_parameters）的输出\n",
    "        \n",
    "    返回：\n",
    "        A2 使用sigmide()函数计算的第二次激活后的数值\n",
    "        cache 包含“Z1”，“Z2”，“A1”和“A2”的字典类型变量\n",
    "    \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # 前向传播计算A2\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    # 使用断言判断格式\n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    cache = {\"Z1\" : Z1,\n",
    "             \"Z2\" : Z2,\n",
    "             \"A1\" : A1,\n",
    "             \"A2\" : A2}\n",
    "    return (A2, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算交叉熵损失\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        A2 使用sigmoid()函数计算的第二次激活后的值\n",
    "        Y 标签向量，维度为（1， 数量）\n",
    "        parameters 包含W1,b1,W2和b2的字典类型的变量\n",
    "    返回：\n",
    "        成本 交叉熵损失\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    # 计算成本\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), (1-Y))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    cost = float(np.squeeze(cost))\n",
    "    \n",
    "    assert(isinstance(cost, float))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向传播\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        parameters 包含参数的一个字典型变量\n",
    "        cache 包含“Z1”，“A1”，“Z2”和“A2”的字典类型的变量\n",
    "        X 输入数据，维度为（2， 数量）\n",
    "        Y 标签，维度为（1，数量）\n",
    "    返回\n",
    "        grads 包含W和b的一个字典型变量\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    grads = {\"dW1\" : dW1,\n",
    "             \"db1\" : db1,\n",
    "             \"dW2\" : dW2,\n",
    "             \"db2\" : db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新参数\n",
    "def update_parameters(parameters, grads, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        parameters 包含参数的字典类型变量\n",
    "        grads 包含导数值的字典类型变量\n",
    "        learning_rate 学习率\n",
    "    返回：\n",
    "        parameters 包含更新参数后的字典类型变量\n",
    "    \"\"\"\n",
    "    \n",
    "    W1, W2 = parameters[\"W1\"], parameters[\"W2\"]\n",
    "    b1, b2 = parameters[\"b1\"], parameters[\"b2\"]\n",
    "    \n",
    "    dW1, dW2 = grads[\"dW1\"], grads[\"dW2\"]\n",
    "    db1, db2 = grads[\"db1\"], grads[\"db2\"]\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters={\"W1\" : W1,\n",
    "                \"b1\" : b1,\n",
    "                \"W2\" : W2,\n",
    "                \"b2\" : b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations, print_cost = False):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        X 数据集，维度为（2， 示例数）\n",
    "        Y 标签，维度为（1， 示例数）\n",
    "        n_h 隐藏层的数量\n",
    "        num_iterations 梯度下降循环中的循环次数\n",
    "        print_cost 如果为True，则每1000次迭代打印一次成本数值\n",
    "    返回：\n",
    "        parameters 学习模型的参数，用它们来进行预测\n",
    "    \"\"\"\n",
    "    np.random.seed(3) # 指定随机种子\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate=0.005)\n",
    "        \n",
    "        if print_cost:\n",
    "            if i%1000 == 0:\n",
    "                print(\"第 \", i, \"次循环，lost为\" + str(cost))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        parameters 包含参数的字典类型的变量\n",
    "        X 输入数据（n_x, m）\n",
    "    返回：\n",
    "        prediction 模型预测的向量（红色：0/蓝色：1）\n",
    "    \"\"\"\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    prediction = np.round(A2)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第  0 次循环，lost为0.6842114074142202\n",
      "第  1000 次循环，lost为0.3264313395846262\n",
      "第  2000 次循环，lost为0.32831877139820115\n",
      "第  3000 次循环，lost为0.28471908477146934\n",
      "第  4000 次循环，lost为0.27301584460614314\n",
      "第  5000 次循环，lost为0.21002688236507153\n",
      "第  6000 次循环，lost为0.1598817366481211\n",
      "第  7000 次循环，lost为0.10728698473875664\n",
      "第  8000 次循环，lost为0.05174990823928235\n",
      "第  9000 次循环，lost为-0.00872881451005084\n",
      "第  10000 次循环，lost为-0.07573264323875746\n",
      "第  11000 次循环，lost为-0.07940193257707871\n",
      "第  12000 次循环，lost为-0.17309869805181946\n",
      "第  13000 次循环，lost为-0.2902741790741063\n",
      "第  14000 次循环，lost为-0.3878183641409032\n",
      "第  15000 次循环，lost为-0.4870426138718867\n",
      "第  16000 次循环，lost为-0.5880853573225145\n",
      "第  17000 次循环，lost为-0.6916338254807224\n",
      "第  18000 次循环，lost为-0.7982343976688403\n",
      "第  19000 次循环，lost为-0.9083090302106729\n",
      "第  20000 次循环，lost为-1.0220323004180194\n",
      "第  21000 次循环，lost为-1.1392040955800904\n",
      "第  22000 次循环，lost为-1.2590234368001054\n",
      "第  23000 次循环，lost为-1.3807892556555084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第  24000 次循环，lost为-inf\n",
      "第  25000 次循环，lost为-inf\n",
      "第  26000 次循环，lost为-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第  27000 次循环，lost为nan\n",
      "第  28000 次循环，lost为nan\n",
      "第  29000 次循环，lost为nan\n",
      "第  30000 次循环，lost为nan\n",
      "第  31000 次循环，lost为nan\n",
      "第  32000 次循环，lost为nan\n",
      "第  33000 次循环，lost为nan\n",
      "第  34000 次循环，lost为nan\n",
      "第  35000 次循环，lost为nan\n",
      "第  36000 次循环，lost为nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:32: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第  37000 次循环，lost为nan\n",
      "第  38000 次循环，lost为nan\n",
      "第  39000 次循环，lost为nan\n",
      "第  40000 次循环，lost为nan\n",
      "第  41000 次循环，lost为nan\n",
      "第  42000 次循环，lost为nan\n",
      "第  43000 次循环，lost为nan\n",
      "第  44000 次循环，lost为nan\n",
      "第  45000 次循环，lost为nan\n",
      "第  46000 次循环，lost为nan\n",
      "第  47000 次循环，lost为nan\n",
      "第  48000 次循环，lost为nan\n",
      "第  49000 次循环，lost为nan\n",
      "准确率：48.529411764705884 %\n"
     ]
    }
   ],
   "source": [
    "# 正式运行\n",
    "def sigmoid(x):\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "parameters = nn_model(X, Y, n_h=4, num_iterations=50000,print_cost=True)\n",
    "predictions = predict(parameters, X)\n",
    "accuracy = precision_score(Y[0], predictions[0], average='micro')*100\n",
    "print(\"准确率：{} %\" .format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 85  0  0]\n",
      " [ 0 82  0  0]\n",
      " [ 0 14  0  0]\n",
      " [ 0 23  0  0]]\n",
      "隐藏层的节点数量： 1， 准确率: 40.19607843137255 %\n",
      "[[ 4 81  0  0]\n",
      " [ 1 81  0  0]\n",
      " [ 0 14  0  0]\n",
      " [ 0 23  0  0]]\n",
      "隐藏层的节点数量： 2， 准确率: 41.66666666666667 %\n",
      "[[17 68  0  0]\n",
      " [ 5 77  0  0]\n",
      " [ 0 14  0  0]\n",
      " [ 1 22  0  0]]\n",
      "隐藏层的节点数量： 3， 准确率: 46.07843137254902 %\n",
      "[[18 67  0  0]\n",
      " [ 3 79  0  0]\n",
      " [ 0 14  0  0]\n",
      " [ 2 21  0  0]]\n",
      "隐藏层的节点数量： 4， 准确率: 47.549019607843135 %\n",
      "[[20 65  0  0]\n",
      " [ 6 76  0  0]\n",
      " [ 0 14  0  0]\n",
      " [ 2 21  0  0]]\n",
      "隐藏层的节点数量： 5， 准确率: 47.05882352941176 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17 68  0  0]\n",
      " [ 3 79  0  0]\n",
      " [ 0 14  0  0]\n",
      " [ 1 22  0  0]]\n",
      "隐藏层的节点数量： 20， 准确率: 47.05882352941176 %\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 20] #隐藏层数量\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    parameters = nn_model(X, Y, n_h, num_iterations=20000)\n",
    "    predictions = predict(parameters, X)\n",
    "    matrix = confusion_matrix(Y[0], predictions[0])\n",
    "    print(matrix)\n",
    "#     accuracy = float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100)\n",
    "#     accuracy = np.sum(np.not_equal(Y, predictions))/float(Y.size)\n",
    "    accuracy = precision_score(Y[0], predictions[0], average='micro')*100\n",
    "    print (\"隐藏层的节点数量： {}， 准确率: {} %\".format(n_h, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3",
   "language": "python",
   "name": "pytorch3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
